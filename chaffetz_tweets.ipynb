{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Jason Chaffetz' Legacy on Twitter\n",
    "\n",
    "By [Sean McMinn](\"http://www.github.com/shmcminn\")\n",
    "\n",
    "As Utah Republican Rep. Jason Chaffetz prepared to retire on June 30, 2017, Roll Call used this script to parse his most recent tweets, up to the Twitter API cap, then analyze them. Roll Call published the [graphic story](http://www.rollcall.com/news/politics/measuring-chaffetzs-legacy-twitter) using this analaysis on June 29. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"1idCovKXB8ohdRRXtd64jRIxJ\"\n",
    "consumer_secret = \"OqNSyVWG2U6jFiN3VzqeQQ2F7u7QrtC0ouSY8OHhNzs0S6JwJP\"\n",
    "access_key = \"206260986-EsngiVAeiRLA9OKuYb7DRS0hpVLHCPvlNVLWwy7d\"\n",
    "access_secret = \"y7RPDd3Pz9MsLwnqssvlRkYqXtU2xylR2nnCD5F3guvYe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#func from https://gist.github.com/yanofsky/5436496\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "\t#Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\t\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\tauth.set_access_token(access_key, access_secret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no tweets left to grab\n",
    "\twhile len(new_tweets) > 0:\n",
    "\t\tprint(\"getting tweets before %s\" % (oldest))\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\tprint(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\t\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str, tweet.created_at, tweet.text, tweet.retweet_count, tweet.favorite_count] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "\twith open('jasoninthehouse_tweets.csv', 'w') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\twriter.writerow([\"id\",\"created_at\",\"text\", \"RT\", \"fav\"])\n",
    "\t\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get  recent tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 835723037433446399\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 803675892387983360\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 769307103857029119\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 740222520352210943\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 714582426115158015\n",
      "...1199 tweets downloaded so far\n",
      "getting tweets before 691635845703413759\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 669566901832450047\n",
      "...1599 tweets downloaded so far\n",
      "getting tweets before 642941779642159103\n",
      "...1798 tweets downloaded so far\n",
      "getting tweets before 620226797007867907\n",
      "...1998 tweets downloaded so far\n",
      "getting tweets before 594247398869901311\n",
      "...2198 tweets downloaded so far\n",
      "getting tweets before 570423171880497152\n",
      "...2398 tweets downloaded so far\n",
      "getting tweets before 547836488344698879\n",
      "...2598 tweets downloaded so far\n",
      "getting tweets before 514164386697469951\n",
      "...2798 tweets downloaded so far\n",
      "getting tweets before 489469164017704959\n",
      "...2998 tweets downloaded so far\n",
      "getting tweets before 468843586407395327\n",
      "...3198 tweets downloaded so far\n",
      "getting tweets before 426537647817752575\n",
      "...3210 tweets downloaded so far\n",
      "getting tweets before 425470374638145535\n",
      "...3210 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t#pass in the username of the account you want to download\n",
    "\tget_all_tweets(\"jasoninthehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local CSV of tweets downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open csv file as list of dicts\n",
    "\n",
    "with open('jasoninthehouse_tweets.csv') as f:\n",
    "    all_tweets = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make changes to tweet data\n",
    "\n",
    "for twt in all_tweets:\n",
    "    time_minus_four = (datetime.datetime.strptime(twt[\"created_at\"], \"%Y-%m-%d %H:%M:%S\")- datetime.timedelta(hours=4))\n",
    "    twt[\"new_date\"] = str(time_minus_four.date())\n",
    "    twt[\"new_time\"] = str(time_minus_four.time())\n",
    "    twt[\"hour\"] = twt[\"new_time\"][0:2]\n",
    "    twt[\"weekday\"] = time_minus_four.weekday()\n",
    "    if twt[\"weekday\"] is 7:\n",
    "        twt[\"weekday\"] = 0\n",
    "    twt[\"weekday\"] = twt[\"weekday\"] + 1\n",
    "    twt[\"text\"] = twt[\"text\"].replace(\"b'\",\"\")\n",
    "    twt[\"text\"] = twt[\"text\"].replace('b\"',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write csv from new tweet data\n",
    "\n",
    "out_file_name = \"jasoninthehouse_tweets.csv\"\n",
    "\n",
    "with open(out_file_name, \"w\") as csvfile:\t\n",
    "\tfieldnames = all_tweets[0].keys()\n",
    "\twriter = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "\twriter.writeheader()\n",
    "\tfor el in all_tweets:\n",
    "\t\twriter.writerow(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most popular tweets\n",
    "\n",
    "## Most retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fb873d96e19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get top 10 RT'd tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmost_rts\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# get top 10 RT'd tweets\n",
    "\n",
    "most_rts =  sorted(all_tweets, key=lambda k: int(k['RT']), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most favorited tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RT': '21835',\n",
       "  'created_at': '2016-10-28 16:57:17',\n",
       "  'fav': '29692',\n",
       "  'hour': '12',\n",
       "  'id': '792047597040971776',\n",
       "  'new_date': '2016-10-28',\n",
       "  'new_time': '12:57:17',\n",
       "  'text': 'FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened',\n",
       "  'weekday': 5},\n",
       " {'RT': '6162',\n",
       "  'created_at': '2017-05-16 22:58:45',\n",
       "  'fav': '14286',\n",
       "  'hour': '18',\n",
       "  'id': '864616135466950656',\n",
       "  'new_date': '2017-05-16',\n",
       "  'new_time': '18:58:45',\n",
       "  'text': '.@GOPoversight is going to get the Comey memo, if it exists. I need to see it sooner rather than later. I have my subpoena pen ready.',\n",
       "  'weekday': 2},\n",
       " {'RT': '11545',\n",
       "  'created_at': '2016-11-06 20:15:36',\n",
       "  'fav': '10761',\n",
       "  'hour': '16',\n",
       "  'id': '795358997083586560',\n",
       "  'new_date': '2016-11-06',\n",
       "  'new_time': '16:15:36',\n",
       "  'text': 'FBI Dir just informed us \"Based on our review, we have not changed our conclusions that we expressed in July with respect to Sec Clinton\"',\n",
       "  'weekday': 7},\n",
       " {'RT': '5903',\n",
       "  'created_at': '2016-10-27 01:05:55',\n",
       "  'fav': '10107',\n",
       "  'hour': '21',\n",
       "  'id': '791445788656226304',\n",
       "  'new_date': '2016-10-26',\n",
       "  'new_time': '21:05:55',\n",
       "  'text': 'I will not defend or endorse @realDonaldTrump, but I am voting for him.  HRC is that bad.  HRC is bad for the USA.',\n",
       "  'weekday': 3},\n",
       " {'RT': '4095',\n",
       "  'created_at': '2017-03-02 13:14:39',\n",
       "  'fav': '9215',\n",
       "  'hour': '09',\n",
       "  'id': '837290054385033218',\n",
       "  'new_date': '2017-03-02',\n",
       "  'new_time': '09:14:39',\n",
       "  'text': 'AG Sessions should clarify his testimony and recuse himself',\n",
       "  'weekday': 4},\n",
       " {'RT': '1852',\n",
       "  'created_at': '2017-02-02 05:07:01',\n",
       "  'fav': '6846',\n",
       "  'hour': '01',\n",
       "  'id': '827020474433544192',\n",
       "  'new_date': '2017-02-02',\n",
       "  'new_time': '01:07:01',\n",
       "  'text': \"I am withdrawing HR 621. I'm a proud gun owner, hunter and love our public lands. The bill would… https://t.co/FLhLaiAzkw\",\n",
       "  'weekday': 4},\n",
       " {'RT': '3931',\n",
       "  'created_at': '2016-09-27 16:24:10',\n",
       "  'fav': '5942',\n",
       "  'hour': '12',\n",
       "  'id': '780805240887111682',\n",
       "  'new_date': '2016-09-27',\n",
       "  'new_time': '12:24:10',\n",
       "  'text': 'Today I filed a privileged report to hold Bryan Pagliano in contempt of Congress #subpoenasarenotoptional https://t.co/WkJZWu8oOq',\n",
       "  'weekday': 2},\n",
       " {'RT': '3868',\n",
       "  'created_at': '2017-05-22 21:39:09',\n",
       "  'fav': '5475',\n",
       "  'hour': '17',\n",
       "  'id': '866770432434741248',\n",
       "  'new_date': '2017-05-22',\n",
       "  'new_time': '17:39:09',\n",
       "  'text': 'Spoke with Comey. He wants to speak with Special Counsel prior to public testimony. Hearing Wed postponed. @GOPoversight',\n",
       "  'weekday': 1},\n",
       " {'RT': '2143',\n",
       "  'created_at': '2017-02-09 22:23:31',\n",
       "  'fav': '4860',\n",
       "  'hour': '18',\n",
       "  'id': '829818033224900608',\n",
       "  'new_date': '2017-02-09',\n",
       "  'new_time': '18:23:31',\n",
       "  'text': 'What she did was wrong, wrong, wrong. Here is our bi-partisan letter to the White House and OGE. #Donteverdothis  https://t.co/zqeYhcttMB',\n",
       "  'weekday': 4},\n",
       " {'RT': '1081',\n",
       "  'created_at': '2016-11-09 15:43:50',\n",
       "  'fav': '4182',\n",
       "  'hour': '11',\n",
       "  'id': '796377766275018752',\n",
       "  'new_date': '2016-11-09',\n",
       "  'new_time': '11:43:50',\n",
       "  'text': 'President-elect Trump!  Congratulations!',\n",
       "  'weekday': 3}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top 10 fav'd tweets\n",
    "\n",
    "most_favs =  sorted(all_tweets, key=lambda k: int(k['fav']), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out what he's tweeting about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get top 100 word freq count\n",
    "\n",
    "all_words = \"\"\n",
    "for twt in all_tweets:\n",
    "    all_words = all_words + \" \" + twt[\"text\"]\n",
    "    \n",
    "words_freq =Counter(all_words.lower().replace(\":\",\"\").split()).most_common()\n",
    "    \n",
    "most_used_words = Counter(all_words.lower().split()).most_common()[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most used hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#benghazi - 38\n",
      "#tbt - 35\n",
      "#utah - 34\n",
      "#utpol - 29\n",
      "#irs - 26\n",
      "#tcot - 20\n",
      "#fastandfurious - 14\n",
      "#ut3rddistrict - 14\n",
      "#irsfail - 11\n",
      "#usa - 9\n",
      "#secretservice - 8\n",
      "#neverforget - 7\n",
      "#breaking - 7\n",
      "#emerycounty - 7\n",
      "#nomidnightmonument - 6\n",
      "#epa - 6\n",
      "#nature - 5\n",
      "#july4 - 5\n",
      "#americanforkcanyon - 4\n",
      "#taxday - 4\n",
      "#oneluckyguy - 4\n",
      "#plannedparenthood - 4\n",
      "#byufootball - 4\n",
      "#gopoversight - 4\n",
      "#natgeo - 4\n",
      "#sxsw - 4\n"
     ]
    }
   ],
   "source": [
    "# get most used hashtags\n",
    "\n",
    "hashtags_freq = []\n",
    "\n",
    "most_used_hashtags = []\n",
    "\n",
    "for word in words_freq:\n",
    "    if word[0][0] == \"#\":\n",
    "        hashtags_freq.append(word)\n",
    "        if word[1] > 3:\n",
    "            most_used_hashtags.append(word)\n",
    "            \n",
    "# fix utah\n",
    "\n",
    "for ind, ht in enumerate(most_used_hashtags):\n",
    "    if ht[0] == \"#utah…\":\n",
    "        add_utah_num = ht[1]\n",
    "        most_used_hashtags.pop(ind)\n",
    "        \n",
    "\n",
    "for ind,ht in enumerate(most_used_hashtags):\n",
    "    if ht[0] == \"#utah\":\n",
    "        most_used_hashtags[ind] = [\"#utah\", ht[1]+add_utah_num]\n",
    "  \n",
    "for ht in most_used_hashtags:\n",
    "    print(ht[0] + \" - \" + str(ht[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get users he tweets @ the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@gopoversight - 505\n",
      "@foxnews - 165\n",
      "@youtube - 51\n",
      "@jasoninthehouse - 49\n",
      "@politico - 41\n",
      "@cnn - 37\n",
      "@kslnewsradio - 37\n",
      "@sltrib - 31\n",
      "@speakerryan - 30\n",
      "@usatoday - 27\n",
      "@wsj - 25\n",
      "@washingtonpost - 24\n",
      "@greta - 23\n",
      "@abc - 22\n",
      "@kslcom - 21\n",
      "@dailycaller - 21\n",
      "@dougwrightshow - 18\n",
      "@tgowdysc - 16\n",
      "@wolfblitzer - 15\n",
      "@speakerboehner - 15\n"
     ]
    }
   ],
   "source": [
    "# get most used ats\n",
    "\n",
    "ats_freq = []\n",
    "\n",
    "most_used_ats = []\n",
    "\n",
    "for word in words_freq:\n",
    "    if word[0][0] == \"@\" and word[0] != \"@\":\n",
    "        ats_freq.append(word)\n",
    "        if word[1] > 14:\n",
    "            most_used_ats.append(word)\n",
    "  \n",
    "for at in most_used_ats:\n",
    "    print(at[0] + \" - \" + str(at[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Twitter data on other members who entered House at same time as Chaffetz ('09)\n",
    "\n",
    "We used this to see if Chaffetz was the most active among his classmates on Twitter. He was.\n",
    "\n",
    "Twitter usernames compiled by [congress-legislators](https://github.com/unitedstates/congress-legislators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished JasonInTheHouse\n",
      "finished RepMikeCoffman\n",
      "finished RepGuthrie\n",
      "finished GreggHarper\n",
      "finished Rep_Hunter\n",
      "finished RepLynnJenkins\n",
      "finished RepLanceNJ7\n",
      "finished RepBlainePress\n",
      "finished RepMcClintock\n",
      "finished RepPeteOlson\n",
      "finished RepErikPaulsen\n",
      "finished CongBillPosey\n",
      "finished DrPhilRoe\n",
      "finished RepRooney\n",
      "finished CongressmanGT\n"
     ]
    }
   ],
   "source": [
    "# scrape twitter vitals for chaffetz' class in House\n",
    "\n",
    "\n",
    "member_uns = [\"JasonInTheHouse\",\n",
    "\"RepMikeCoffman\",\n",
    "\"RepGuthrie\",\n",
    "\"GreggHarper\",\n",
    "\"Rep_Hunter\",\n",
    "\"RepLynnJenkins\",\n",
    "\"RepLanceNJ7\",\n",
    "\"RepBlainePress\",\n",
    "\"RepMcClintock\",\n",
    "\"RepPeteOlson\",\n",
    "\"RepErikPaulsen\",\n",
    "\"CongBillPosey\",\n",
    "\"DrPhilRoe\",\n",
    "\"RepRooney\",\n",
    "\"CongressmanGT\"]\n",
    "\n",
    "class_data = []\n",
    "\n",
    "url_prefix = \"http://www.twitter.com/\"\n",
    "\n",
    "for un in member_uns:\n",
    "    resp = requests.get(url_prefix + un)\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    vitals = soup.find_all(\"a\", {\"class\":\"ProfileNav-stat\"})\n",
    "    d = {}\n",
    "    for vital in vitals:\n",
    "        d[vital.find_all(\"span\")[0].getText()] = vital.find_all(\"span\")[2].getText()  \n",
    "    d[\"un\"] = un\n",
    "    class_data.append(d)\n",
    "    print(\"finished \" + un)\n",
    "    time.sleep(2) #seconds\n",
    "    \n",
    "longest_vital = [0,0]    \n",
    "\n",
    "for ind, member in enumerate(class_data):\n",
    "    member[\"Tweets\"] = member[\"Tweets\"].replace(\"\\n\",\"\").strip()\n",
    "    if len(member.keys()) > longest_vital[1]:\n",
    "        longest_vital[0] = ind\n",
    "        longest_vital[1] = len(member.keys())\n",
    "    \n",
    "    \n",
    "out_file_name = \"class_09_twitter_vitals.csv\"\n",
    "\n",
    "with open(out_file_name, \"w\") as csvfile:\n",
    "    fieldnames = class_data[longest_vital[0]].keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "    for el in class_data:\n",
    "        writer.writerow(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
